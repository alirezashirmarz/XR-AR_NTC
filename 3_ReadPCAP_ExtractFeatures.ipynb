{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezashirmarz/XR-AR_NTC/blob/main/3_ReadPCAP_ExtractFeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwoK6A-bCHQJ"
      },
      "source": [
        "# **Title**: Extract IPI, FS, and IFI Features from PCAP(NJ) File\n",
        "\n",
        "> Indented block\n",
        "    ## Pay attention to configuration in each cell"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "7mzoKM6nCHQO"
      },
      "source": [
        "# install prerequirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V1jo6taCHQP"
      },
      "outputs": [],
      "source": [
        "# Install Scapy package\n",
        "\n",
        "!pip install scapy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHtD9EbZCHQQ"
      },
      "source": [
        "# 1- Find the Files in the root directory e.g. pcap or pcapnj files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec25rV28CHQQ"
      },
      "outputs": [],
      "source": [
        "# Set the root directory includes PCAP(NJ) files\n",
        "\n",
        "root_directory = r'/home/alireza/myframes/Uplink'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ9fDZxhCHQR",
        "outputId": "86551cfe-3976-4f98-a044-6d63182f081c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files found:\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-6.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-5.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-7.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-4.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-10.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-8.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-3.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-1.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-2.pcap\n",
            "/home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-9.pcap\n"
          ]
        }
      ],
      "source": [
        "# Find all pcap files in  a directory\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def find_files_with_extension(root_dir, extension):\n",
        "    file_list = []\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(extension):\n",
        "                file_list.append(os.path.join(root, file))\n",
        "    return file_list\n",
        "\n",
        "# Example usage\n",
        "extension = '.pcap'  # Possible values --> '.pcap' or  '.pcapng'\n",
        "files_with_extension = find_files_with_extension(root_directory, extension)\n",
        "\n",
        "print(\"Files found:\")\n",
        "for file_path in files_with_extension:\n",
        "    print(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "z3LEQ-thCHQS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHVdmY1MCHQV"
      },
      "source": [
        "# 2- Extract the Features (IPI,IFI, FS, & PS) of flow from PCAP(NJ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoV92q24CHQV",
        "outputId": "0270fbae-56ef-476e-c66d-e0222149f496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset 1: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-6.pcap.csv was stored!\n",
            "The dataset 2: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-5.pcap.csv was stored!\n",
            "The dataset 3: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-7.pcap.csv was stored!\n",
            "The dataset 4: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-4.pcap.csv was stored!\n",
            "The dataset 5: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-10.pcap.csv was stored!\n",
            "The dataset 6: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-8.pcap.csv was stored!\n",
            "The dataset 7: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-3.pcap.csv was stored!\n",
            "The dataset 8: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-1.pcap.csv was stored!\n",
            "The dataset 9: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-2.pcap.csv was stored!\n",
            "The dataset 10: /home/alireza/myframes/5-stream2-3840-1920-72-60/Stream2-3840-72-9.pcap.csv was stored!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import scapy.all as scapy\n",
        "from collections import defaultdict\n",
        "\n",
        "def process_pcap(pcap_file, csv_file):\n",
        "    packets = scapy.rdpcap(pcap_file)\n",
        "    flows = defaultdict(list)\n",
        "\n",
        "    # Process packets and organize them into UDP/QUIC flows\n",
        "    for packet in packets:\n",
        "        if packet.haslayer(scapy.UDP) or \"quic\" in str(packet).lower():  # Heuristic for QUIC\n",
        "            ip_layer = packet[scapy.IP] if packet.haslayer(scapy.IP) else packet[scapy.IPv6]\n",
        "            protocol = 'QUIC' if \"quic\" in str(packet).lower() else 'UDP'\n",
        "            flow_key = (\n",
        "                ip_layer.src, ip_layer.dst,\n",
        "                packet[scapy.UDP].sport, packet[scapy.UDP].dport, ip_layer.name, protocol\n",
        "            )\n",
        "            flows[flow_key].append(packet)\n",
        "\n",
        "    with open(csv_file, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            'ID', 'SrcIP', 'DstIP', 'SrcPort', 'DstPort',\n",
        "            'IPVersion', 'Protocol', 'PS', 'IPI', 'FlowSizeBytes',\n",
        "            'FlowSizePackets', 'FS', 'FS(PKT)',\n",
        "            'NumFrames', 'IFI'\n",
        "        ])\n",
        "\n",
        "        for i, (flow_key, packets) in enumerate(flows.items()):\n",
        "            src_ip, dst_ip, src_port, dst_port, ip_version, protocol = flow_key\n",
        "\n",
        "            timestamps = [packet.time for packet in packets]\n",
        "            ip_intervals = [j - i for i, j in zip(timestamps[:-1], timestamps[1:])]\n",
        "            packet_sizes = [len(packet) for packet in packets]\n",
        "            total_flow_bytes = sum(packet_sizes)\n",
        "            total_flow_packets = len(packets)\n",
        "\n",
        "            avg_packet_size = total_flow_bytes / total_flow_packets if total_flow_packets else 0\n",
        "            avg_ipi = sum(ip_intervals) / len(ip_intervals) if ip_intervals else 0\n",
        "\n",
        "            # Frame Analysis (specific to UDP, might not be accurate for QUIC)\n",
        "            frame_sizes_bytes, frame_sizes_packets, frame_intervals = analyze_frames(packets)\n",
        "\n",
        "            avg_frame_size_bytes = sum(frame_sizes_bytes) / len(frame_sizes_bytes) if frame_sizes_bytes else 0\n",
        "            avg_frame_size_packets = sum(frame_sizes_packets) / len(frame_sizes_packets) if frame_sizes_packets else 0\n",
        "            num_frames = len(frame_sizes_bytes)\n",
        "            avg_ifi = sum(frame_intervals) / len(frame_intervals) if frame_intervals else 0\n",
        "\n",
        "            # Writing to CSV\n",
        "            writer.writerow([\n",
        "                i + 1, src_ip, dst_ip, src_port, dst_port, ip_version, protocol,\n",
        "                avg_packet_size, avg_ipi, total_flow_bytes, total_flow_packets,\n",
        "                avg_frame_size_bytes, avg_frame_size_packets, num_frames, avg_ifi\n",
        "            ])\n",
        "\n",
        "def analyze_frames(packets):\n",
        "    frame_sizes_bytes = []\n",
        "    frame_sizes_packets = []\n",
        "    frame_intervals = []\n",
        "    current_frame_size_bytes = 0\n",
        "    current_frame_size_packets = 0\n",
        "    current_frame_start_time = None\n",
        "    last_udp_length = None\n",
        "\n",
        "    for packet in packets:\n",
        "        if packet.haslayer(scapy.UDP):\n",
        "            udp_length = packet[scapy.UDP].len\n",
        "            if current_frame_start_time is None:\n",
        "                current_frame_start_time = packet.time\n",
        "\n",
        "            if last_udp_length is not None and udp_length != last_udp_length:\n",
        "                frame_sizes_bytes.append(current_frame_size_bytes)\n",
        "                frame_sizes_packets.append(current_frame_size_packets)\n",
        "                frame_intervals.append(packet.time - current_frame_start_time)\n",
        "                current_frame_size_bytes = 0\n",
        "                current_frame_size_packets = 0\n",
        "                current_frame_start_time = packet.time\n",
        "\n",
        "            current_frame_size_bytes += len(packet)\n",
        "            current_frame_size_packets += 1\n",
        "            last_udp_length = udp_length\n",
        "\n",
        "    if current_frame_size_bytes > 0:\n",
        "        frame_sizes_bytes.append(current_frame_size_bytes)\n",
        "        frame_sizes_packets.append(current_frame_size_packets)\n",
        "\n",
        "    return frame_sizes_bytes, frame_sizes_packets, frame_intervals\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "\n",
        "def main(pcap_file, csv_file):\n",
        "    #first_five_rows = extract_and_save_data(pcap_file, csv_file)\n",
        "    process_pcap(pcap_file, csv_file + '.csv')\n",
        "\n",
        "    #for row in first_five_rows:\n",
        "        #print(row)\n",
        "i = 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace these with the actual file paths\n",
        "    for addr in files_with_extension:\n",
        "\n",
        "        #pcap_path = \"E:\\Postdoc_UFScar\\Dataset\\Other Datasets\\VOD\\\\orangejuice.pcapng\"\n",
        "        i=i+1\n",
        "        csv_path = addr+'.csv'                          #\"E:\\\\Postdoc_UFScar\\\\Dataset\\\\Other Datasets\\\\game\\\\orangejuice.csv\"\n",
        "        main(addr, csv_path)\n",
        "        print(f'The dataset {i}: {csv_path} was stored!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raV30Nw2CHQX"
      },
      "source": [
        "# 3- Read the CSV and Merge Them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zhP0qbxCHQX",
        "outputId": "6dde6b54-f8b6-4768-92ba-e0d8da5dc281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/alireza/myframes/Uplink\n",
            "The df with 30 rows and 16 columns was stored as csv file!\n"
          ]
        }
      ],
      "source": [
        "# Read and merge the csv files\n",
        "\n",
        "'''\n",
        "# Configuration\n",
        "    # Input:\n",
        "      1- root-directory--> It is the directory with csv files (In this case, it\n",
        "      uses the roo directory from 2 previous cell)\n",
        "      2- ds_name --> the name you want to save the csv file (without extension)\n",
        "    # Output:\n",
        "      1- Save the\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "#root_directory = \"E:\\Postdoc_UFScar\\Dataset\\Other Datasets\\VOD\"\n",
        "ds_name= 'DS'\n",
        "print(root_directory)\n",
        "csvfiles = find_files_with_extension(root_directory, 'csv')\n",
        "dff = []\n",
        "for mycsv in csvfiles:\n",
        "    mydf = pd.read_csv(mycsv)\n",
        "    dff.append(mydf)\n",
        "df = pd.concat(dff, ignore_index=True, sort=False)\n",
        "#df = pd.concat(dff, ignore_index=True)\n",
        "df.to_csv(root_directory +'/' + ds_name +'.csv', index=False)\n",
        "#df.to_csv(root_directory + '\\\\AR.csv', index=False)\n",
        "print(f\"The df with {df.shape[0]} rows and {df.shape[1]} columns was stored as csv file!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYOJNjDKCHQY"
      },
      "source": [
        "# 4- Pre-Processing\n",
        "## Add direction for each flow & remove the NAN Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieiKKK-WCHQY"
      },
      "outputs": [],
      "source": [
        "#set processing file address\n",
        "\n",
        "mycsvfile = root_directory + '/DS.csv'\n",
        "Directionfiles = mycsvfile.split('.')[0]+'2'+'.csv'\n",
        "Removedfile = mycsvfile.split('.')[0]+'3'+'.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_eXvCGNCHQY"
      },
      "outputs": [],
      "source": [
        "# Add the direction to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oB7re2FCHQY"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def add_direction_feature(input_file, output_file):\n",
        "    with open(input_file, mode='r', newline='') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        rows = list(reader)\n",
        "\n",
        "    # Keep track of seen destination IPs and ports\n",
        "    seen_destinations = set()\n",
        "\n",
        "    for row in rows:\n",
        "        src_tuple = (row['SrcIP'], row['SrcPort'])\n",
        "        dst_tuple = (row['DstIP'], row['DstPort'])\n",
        "\n",
        "        # Determine direction\n",
        "        if src_tuple in seen_destinations:\n",
        "            row['Direction'] = 'Downlink'\n",
        "        else:\n",
        "            row['Direction'] = 'Uplink'\n",
        "            seen_destinations.add(dst_tuple)\n",
        "\n",
        "    # Write to new CSV file\n",
        "    with open(output_file, mode='w', newline='') as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames + ['Direction'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "# Example usage\n",
        "add_direction_feature(mycsvfile,Directionfiles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBb0QtOTCHQZ"
      },
      "outputs": [],
      "source": [
        "# Remove the flows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp0cTJtzCHQZ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def filter_csv(input_file, output_file):\n",
        "    with open(input_file, mode='r', newline='') as infile, open(output_file, mode='w', newline='') as outfile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "\n",
        "        for row in reader:\n",
        "            num_frames = int(row['NumFrames'])\n",
        "            ifi = float(row['IFI'])\n",
        "\n",
        "            # Keep the row if NumFrames is greater than 1 and IFI is not 0\n",
        "            if num_frames >= 1 and ifi != 0:\n",
        "                writer.writerow(row)\n",
        "\n",
        "# Example usage\n",
        "filter_csv(Directionfiles, Removedfile)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRKS3LCnCHQZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}