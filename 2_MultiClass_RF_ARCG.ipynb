{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDIrHTqCPVJQ"
      },
      "source": [
        "# **(2) Multi-Class Random Forest (RF) Classification Model of AR, CG and Others (Uplink/Downlink)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM6axv_xPVJR"
      },
      "source": [
        "# 1- **Read the Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtfSdV-EPVJR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "# Load datasets\n",
        "### Enter your Dataset file address with csv format\n",
        "ar_data = pd.read_csv(r'AR.csv')\n",
        "cg_data = pd.read_csv(r'CG.csv')\n",
        "others_data = pd.read_csv(r'others.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set the features\n",
        "features = ['IPI', 'FS', 'IFI']\n",
        "\n",
        "ar_data = ar_data[features]\n",
        "cg_data = cg_data[features]\n",
        "others_data = others_data[features]\n",
        "\n",
        "# Label the datasets\n",
        "ar_data['Class'] = 'AR'\n",
        "cg_data['Class'] = 'CG'\n",
        "others_data['Class'] = 'Others'\n",
        "\n",
        "\n",
        "# Show the number of Samples in training Dataset\n",
        "print(f'AR samples are {ar_data.shape[0]}\\nCG samples are {cg_data.shape[0]}\\nother samples are {others_data.shape[0]}\\n###########')\n",
        "\n",
        "\n",
        "\n",
        "# Combine and shuffle the samples (Dataset of Combined Samples of AR & CG & other APPs (DS_CSACO))\n",
        "DS_CSACO = pd.concat([ar_data, cg_data, others_data]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "pprint(f'The #No of samples belong to AR is ' + str(DS_CSACO[DS_CSACO['Class']=='AR'].shape[0]))\n",
        "print(f'The #No of samples belong to CG is ' + str(DS_CSACO[DS_CSACO['Class']=='CG'].shape[0]))\n",
        "print(f'The #No of samples belong to Other Apps is ' + str(DS_CSACO[DS_CSACO['Class']=='Others'].shape[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IujQKkvPVJS"
      },
      "source": [
        "# 2- **Train the AR and CG model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVz4fU2APVJT"
      },
      "outputs": [],
      "source": [
        "# Train DT model\n",
        "# Split the data\n",
        "X = combined_data[features]\n",
        "y = combined_data['Class']\n",
        "\n",
        "# Class weight\n",
        "class_weights = {'AR': 1 / combined_data[combined_data['Class']=='AR'].shape[0],\n",
        "                 'CG': 1 / combined_data[combined_data['Class']=='CG'].shape[0],\n",
        "                 'Others': 1/combined_data[combined_data['Class']=='Others'].shape[0]}\n",
        "\n",
        "\n",
        "# Train AR model (Testsize = 10%, Classweight, and n_estimators = 100)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=30)\n",
        "rf_model = RandomForestClassifier(n_estimators=100,class_weight=class_weights, random_state=30)\n",
        "rf_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khzw9-9hPVJT"
      },
      "source": [
        "# 3- **Evaluate the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfqlynGFPVJT"
      },
      "outputs": [],
      "source": [
        "# Evluate AR & CG Model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    return accuracy, precision, recall, f1, conf_matrix\n",
        "\n",
        "# Evaluate AR and CG models\n",
        "accuracy, precision, recall, f1, conf = evaluate_model(rf_model, X_test, y_test)\n",
        "\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'AR Model Metrics:accuracy is {accuracy} |\\nPrecision is {precision} |\\nrecall is {recall} |\\nf-score is {f1}', '\\nIt is confusion matrix:\\n', conf)\n",
        "\n",
        "#print(y_test['Class'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3aWGLo9PVJU"
      },
      "outputs": [],
      "source": [
        "# Cross Validation Model Test\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scoring = {'precision': make_scorer(precision_score, average='macro'),\n",
        "           'recall': make_scorer(recall_score, average='macro'),\n",
        "           'f1_score': make_scorer(f1_score, average='macro')}\n",
        "\n",
        "\n",
        "cv_results = cross_validate(rf_model, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "print(\"Precision:\", cv_results['test_precision'].mean())\n",
        "print(\"Recall:\", cv_results['test_recall'].mean())\n",
        "print(\"F1 Score:\", cv_results['test_f1_score'].mean())\n",
        "\n",
        "print(\"**************************************************************\")\n",
        "dt_cv_scores = cross_val_score(rf_model, X, y, cv=10)\n",
        "print(\"Decision Tree - CV Scores (Accuracy):\", dt_cv_scores)\n",
        "print(\"Decision Tree - Average CV Score (Accuracy Mean):\", dt_cv_scores.mean())\n",
        "\n",
        "print(conf)\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgRCy7kaPVJV"
      },
      "source": [
        "# 4- **Improved Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6w_m-NzPVJV"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with your features and labels\n",
        "X = combined_data.drop('Class', axis=1)  # Replace 'label' with your actual label column name\n",
        "y = combined_data['Class']\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 400],  # Number of trees in the forest\n",
        "    'max_features': ['auto', 'sqrt'],  # Number of features to consider at every split\n",
        "    'max_depth': [10, 20, 30, 40, None],   # Maximum number of levels in tree\n",
        "    'min_samples_split': [2, 5, 10],   # Minimum number of samples required to split a node\n",
        "    'min_samples_leaf': [1, 2, 4],     # Minimum number of samples required at each leaf node\n",
        "    'bootstrap': [True, False]         # Method of selecting samples for training each tree\n",
        "}\n",
        "\n",
        "# Initialize GridSearch with cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "rf_model_optimized = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = rf_model_optimized.predict(X)\n",
        "print(classification_report(y, predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y, predictions))\n",
        "confusion = confusion_matrix(y, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HrTi5W8PVJV"
      },
      "source": [
        "# 5- **Test the RF model** *(5-1) load dataset, (5-2) Test model *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1E7AQvWPVJW"
      },
      "source": [
        "### 5-1- **Load the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW1jsSsGPVJW"
      },
      "outputs": [],
      "source": [
        "# Load the test dataset (Mentioned in Table (IV) of the Paper)\n",
        "AR_Test = pd.read_csv(r'AR-Test.csv')\n",
        "CG_Test = pd.read_csv(r'CG_Test.csv')\n",
        "Other_Test = pd.read_csv(r'Other_Test.csv')\n",
        "# ** Notice: Test the Model with direction considering (DL or UL)\n",
        "\n",
        "# Set the features of teh test dataset and label them\n",
        "AR_Test = AR_Test[features]\n",
        "AR_Test['Class'] = 'AR'\n",
        "\n",
        "CG_Test = CG_Test[features]\n",
        "CG_Test['Class'] = 'CG'\n",
        "\n",
        "Other_Test = Other_Test[features]\n",
        "Other_Test['Class'] = 'Others'\n",
        "# ** Notice: Pay attention to the feature variable name to match the features of the test dataset\n",
        "\n",
        "# Combine the Test Dataset\n",
        "DT_Test =  pd.concat([AR_Test,CG_Test,Other_Test],sort=False).sample(frac=1, random_state=30).reset_index(drop=True)\n",
        "\n",
        "\n",
        "X_test = DT_Test[features]\n",
        "y_test = DT_Test['Class']\n",
        "\n",
        "# print the number of each sample in the Test Dataset\n",
        "print('No# of AR samples',DT_Test[DT_Test['Class']=='AR'].shape,\n",
        "      '\\nNo# of CG samples-->',DT_Test[DT_Test['Class']=='CG'].shape,\n",
        "      '\\nNo# of Other Apps samples-->',DT_Test[DT_Test['Class']=='Others'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0ZdCRjYPVJX"
      },
      "source": [
        "# 5-2-**Test rf_model**(without Hyperparameter Tunning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKkAc_AMPVJX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions with the model ************ cls_model & cls_model2\n",
        "predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# print the features used for training\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-2- **Test rf_model_optimized** (with Hyperparameter Tunning)"
      ],
      "metadata": {
        "id": "KdpXMJkBopDx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiMoFOqXPVJX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions with the model ************ rf_model & rf_model2\n",
        "predictions = rf_model_optimized.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# print the features used for training\n",
        "print(features)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}